{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters: Creating Color-Magnitude Diagrams\n",
    "\n",
    "## Background\n",
    "Clusters are groups of stars that are born at the same time. We can plot these stars in *Color vs. Magnitude*, where Color is a difference in the brightness between filters (say, B-V) and Magnitude is the brightness in one filter. This is known as a Color-Magnitude diagram, or a CMD. In this diagram, hotter stars, which are bluer, lie to the left; cooler stars, which are redder, lie to the right. \n",
    "\n",
    "You'll notice that there is a line of stars that form something called the Main Sequence. As a star ages, it begins to fuse more and more of its hydrogen fuel in order to shine; stars that are actively burning hydrogen in their cores fall on the Main Sequence. \n",
    "\n",
    "But, once a star uses up its hydrogen fuel, it evolves off the Main Sequence. When we look at a cluster with, many follow the Main-Sequence turnoff and evolve to become giants, which burn heavier elements in their cores. Brighter and more massive stars burn through their hydrogen fuel faster, while smaller stars burn their fuel more slowly. For this reason, more massive stars will turn off the main sequence faster and we can use the turnoff to measure the age of a cluster. \n",
    "\n",
    "### By the end of this project, you will be able to explain:\n",
    "- the lifecycle of stars\n",
    "- Hertzberg-Russel (HR) and Color-Magnitude diagrams (CMD)\n",
    "- clusters and their ages\n",
    "\n",
    "\n",
    "![image](hrdiagram.png)\n",
    "\n",
    "Credit: eso.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Questions\n",
    "Please watch the following videos and answer these questions below.\n",
    "- Main Sequence and the lifetime of stars: https://www.youtube.com/watch?v=ld75W1dz-h0\n",
    "- Star clusters: https://www.youtube.com/watch?v=an4rgJ3O21A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is the main sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Why are bluer stars hotter and redder stars cooler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between an open and globular cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What kind of stars live the longest on the main-sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What can the main-sequence turnoff tell us about the age of a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Creating a Color-Magnitude Diagram\n",
    "You will create your own color-magnitude diagram (CMD) of a globular cluster and estimate the mass of the stars at its main sequence turnoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reduction\n",
    "from astropy.io import fits \n",
    "import reduction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# for cluster analysis\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.detection import DAOStarFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "### You can either:\n",
    "- Use data from this year (two filters, see the observing log for more info) *or*\n",
    "- Use data from Nickel (2014-06-05) of two globular clusters:\n",
    "    -  bias frames - frames 1-10 \n",
    "    -  V flats - frames 41-45\n",
    "    -  I flats - frames 51-55\n",
    "    -  M92 - frames 85-86 (V, I)\n",
    "    -  M15 - frames 76-77 (V, I)\n",
    "\n",
    "If you want to use data from this year, you will need to instead grab data from the 'data_reduction_activity/data-2023-06-27-nickel' directory. Let me know if you want to do this instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "Before we can use our data for science, we need to reduce it properly, just as we did in the reduction activity. You will be guided through the following steps:\n",
    "- performing an overscan subtraction\n",
    "- performing bias subtraction\n",
    "- performing flat-fielding\n",
    "- performing bad-pixel correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overscan Subtraction\n",
    "An overscan region of the CCD chip that is not exposed to light. The overscan region keeps track of the *bias* level throughout the night; as a reminder, the *bias* is a zero second exposure which tells us how \"bright\" each pixel is without any light hitting the camera. For this reason, the overscan region it is a useful way to remove small variations in the bias level throughout the night. We must first remove these columns and apply an overscan subtraction before we begin on data reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = './data/raw/'\n",
    "files = source_dir + '*.fits'\n",
    "\n",
    "reduction.overscan_subtraction(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Move files to correct directories\n",
    "Having our overscan subtracted data, we can move our files to the correct directories for data reduction. In the data-2023-06-27-nickel directory, you should see a few different folders: \n",
    "- calibration, containing a folder named bias, halpha_flat, r_flat, and v_flat; this folder will store our calibration files.\n",
    "- raw; this folder stores our raw files + overscan subtracted\n",
    "- science\n",
    "\n",
    "In this step, we will use the log and move each file into the correct folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, if your science frames are in files 25-50, then fill the following \"range()\" functions as range(25,51)\n",
    "# (Think, why 51 and not 50?)\n",
    "bias_frames = range() # your bias files here\n",
    "v_flat_frames = range()  # your v flat files here\n",
    "i_flat_frames = range() # your i flat files here\n",
    "\n",
    "science_frames = range() # your science frames here\n",
    "\n",
    "# now we tell it where each of these files live and their names\n",
    "v_flat_files = ['{0}d{1}_os.fits'.format(source_dir, num) for num in v_flat_frames]\n",
    "i_flat_files = ['{0}d{1}_os.fits'.format(source_dir, num) for num in i_flat_frames]\n",
    "bias_files = ['{0}d{1}_os.fits'.format(source_dir, num) for num in bias_frames]\n",
    "science_files = ['{0}d{1}_os.fits'.format(source_dir, num) for num in science_frames]\n",
    "\n",
    "# move over each of the files as defined above to a calibration directory for each of them\n",
    "for file in v_flat_files:\n",
    "    shutil.copy2(file, './data/calibration/v_flat')\n",
    "\n",
    "for file in i_flat_files:\n",
    "    shutil.copy2(file, './data/calibration/i_flat')\n",
    "\n",
    "for file in bias_files:\n",
    "    shutil.copy2(file, './data/calibration/bias')\n",
    "\n",
    "for file in science_files:\n",
    "    shutil.copy2(file, './data/science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create master bias\n",
    "As we said before, the *bias* is a zero second exposure that characterizes the behavoir of our detector when it's not exposed to light; each pixel will have a slightly different bias value. Although the bias level tends to be small, we must remove these extra pixel counts so they don't contribute to our image. To do this, we median combine our bias frames to create a *master bias*. We will then subtract this bias from all images taken with a non-zero exposure, or, our science images and our flats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias files live in the './data/calibration/bias/' directory. We want to also save our median combined bias file, 'bias.fits' in this directory. Fill out the bias_path below with this path and edit the name of the saved file to be 'bias.fits' and then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_path = 'BIASPATH' # fill this out with the bias_path\n",
    "bias_files = glob.glob(bias_path+'*_os.fits') # grab the _os files from this directory\n",
    "data_stack = []\n",
    "\n",
    "for frame in bias_files:\n",
    "    data_stack.append(fits.getdata(frame))\n",
    "\n",
    "# Median combine the bias files to create the master bias frame\n",
    "medianBias = np.median(data_stack,axis=0)\n",
    "\n",
    "# Write out the master bias file (bias.fits) with updated FITS header information\n",
    "header = fits.getheader(bias_files[0])\n",
    "header['HISTORY'] = 'Median combined'\n",
    "fits.writeto(bias_path+'MASTERBIASNAME.fits', medianBias, header, overwrite=True) # edit this to have the correct name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bias subtract all frames\n",
    "Because the bias is in all frames, we must remove it from both our flat frames and our science frames before we can do anything else. I will show an example of how to do this for some of the flat frames (just the b flats), your job will be to do it for the rest of the files. As a note, the bias subtracted frames will be denoted by a _bs.fits to highlight that this frame has had the bias removed already. When you run your cells, check that your bias subtracted files are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to perform the bias subtraction. Check each directory (science directory + calibration). Do you see your bias subtracted files? They should have an _bs extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias subtracting the science frames\n",
    "\n",
    "# Make list of input bias files\n",
    "datafilesin = glob.glob('./data/science/*_os.fits')\n",
    "\n",
    "# _bs stands for bias subtracted in the output file names\n",
    "datafilesout = [i[:-5]+ '_bs.fits' for i in datafilesin]\n",
    "\n",
    "n = len(datafilesin)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(datafilesin[i],header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(datafilesout[i],dataout,header)\n",
    "\n",
    "# Bias subtracting the v_flat frames\n",
    "\n",
    "# Make list of input bias files\n",
    "datafilesin = glob.glob('./data/calibration/v_flat/*_os.fits')\n",
    "\n",
    "# _bs stands for bias subtracted in the output file names\n",
    "datafilesout = [i[:-5]+ '_bs.fits' for i in datafilesin]\n",
    "\n",
    "n = len(datafilesin)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(datafilesin[i],header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(datafilesout[i],dataout,header)\n",
    "\n",
    "# Bias subtracting the i_flat frames\n",
    "\n",
    "# Make list of input bias files\n",
    "datafilesin = glob.glob('./data/calibration/i_flat/*_os.fits')\n",
    "\n",
    "# _bs stands for bias subtracted in the output file names\n",
    "datafilesout = [i[:-5]+ '_bs.fits' for i in datafilesin]\n",
    "\n",
    "n = len(datafilesin)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(datafilesin[i],header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(datafilesout[i],dataout,header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Normalized flat field frames \n",
    "In an ideal system, each pixel would respond the same if the CCD is illuminated uniformly. In reality, though, each pixel has a different response and sensitivity to light, and this response changes filter to filter. A flat-field removes this behavior and divides out the uneven response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create an I flat and a V flat. Below, please edit the names of the V flat to be vflat.fits and the I flat to be iflat.fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the master v flat\n",
    "\n",
    "v_flist = glob.glob('./data/calibration/v_flat/*_bs.fits')\n",
    "vflat_stack = []\n",
    "\n",
    "# Read in each file and normalize by the median\n",
    "for file in v_flist:\n",
    "    data,header = fits.getdata(file,header=True)\n",
    "    data = data / np.median(data)\n",
    "    vflat_stack.append(data)\n",
    "\n",
    "# Median combine the flat fields, then normalize by the mean    \n",
    "vflat = np.median(vflat_stack,axis=0)\n",
    "m = np.mean(vflat)\n",
    "vflat = vflat/m\n",
    "header['HISTORY'] = 'Combined and normalized flat field'\n",
    "fits.writeto('./data/calibration/flats/' + 'FLATNAME.fits', vflat, header) # edit this line \n",
    "\n",
    "##########\n",
    "\n",
    "# create the master i flat\n",
    "\n",
    "i_flist = glob.glob('./data/calibration/i_flat/*_bs.fits')\n",
    "iflat_stack = []\n",
    "\n",
    "# Read in each file and normalize by the median\n",
    "for file in i_flist:\n",
    "    data,header = fits.getdata(file,header=True)\n",
    "    data = data / np.median(data)\n",
    "    iflat_stack.append(data)\n",
    "\n",
    "# Median combine the flat fields, then normalize by the mean    \n",
    "iflat = np.median(iflat_stack,axis=0)\n",
    "m = np.mean(iflat)\n",
    "iflat = iflat/m\n",
    "header['HISTORY'] = 'Combined and normalized flat field'\n",
    "fits.writeto('./data/calibration/flats/' + 'FLATNAME.fits', iflat, header) # edit this line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check one of your flat fields below to make sure they look proper. Does this look like the flat we made in the data reduction activity?\n",
    "\n",
    "If the image doesn't look right, adjust vmin and vmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fits.getdata('./data/calibration/flats/YOURFLAT.fits') # put your flat name here\n",
    "plt.imshow(data, vmin=0, vmax=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Flat field science frames\n",
    "Now that we have our flat frames, we can flat-field our science frames.\n",
    "Choose your V frame and your I frame and type in the overscan subtracted, bias subtracted frame to be flat-fielded.\n",
    "\n",
    "NOTE: In the file names, you will see \"_os.fits\" and \"_os_bs.fits\". These mean \"overscan-subtracted\" and \"overscan-subtracted AND bias-subtracted\", respectively. Which of these files do you want to grab and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v frame\n",
    "\n",
    "# _ff stand for flat fielded for the output file name\n",
    "vdataout = [i[:-5]+ '_ff.fits' for i in ['./data/science/YOURVFRAME.fits']] # edit this line to your frame\n",
    "v_flist = ['./data/science/YOURVFRAME.fits'] # edit this line to your frame\n",
    "\n",
    "# For each file in list, divide by the normalize flat field frame for that filter\n",
    "n=len(v_flist)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(v_flist[i],header=True)\n",
    "    dataout = data / vflat\n",
    "    header['HISTORY'] = 'Flat Fielded'\n",
    "    fits.writeto(vdataout[i],dataout,header)\n",
    "\n",
    "#########\n",
    "\n",
    "# i frame\n",
    "\n",
    "# _ff stand for flat fielded for the output file name\n",
    "idataout = [i[:-5]+ '_ff.fits' for i in ['./data/science/YOURIFRAME.fits']] # edit this line to your frame\n",
    "i_flist = ['./data/science/YOURVFRAME.fits'] # edit this line to your frame\n",
    "\n",
    "# For each file in list, divide by the normalize flat field frame for that filter\n",
    "n=len(i_flist)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(i_flist[i],header=True)\n",
    "    dataout = data / iflat\n",
    "    header['HISTORY'] = 'Flat Fielded'\n",
    "    fits.writeto(idataout[i], dataout, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bad pixel correction\n",
    "Now let's remove known bad columns in the Nickel 1-m CCD from our flat-fielded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in = glob.glob('./data/science/*_ff.fits')\n",
    "files_out = [file[:-5]+'_bp.fits' for file in files_in]\n",
    "\n",
    "reduction.remove_bad_pixels(files_in, files_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check both of your final images before we begin the data analysis. Do they look good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fits.getdata('./data/science/YOURFILEVFILTER.fits') # put your V science file name here\n",
    "plt.imshow(data, vmin=0, vmax=1000)\n",
    "plt.show()\n",
    "\n",
    "data2 = fits.getdata('./data/science/YOURFILEIFILTER.fits') # put your I science file name here\n",
    "plt.imshow(data2, vmin=0, vmax=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "To create our color-magnitude diagram, we first need to find all the stars in both our filters and match them. We can either do this by hand *or* we can use a handy package that automatically identifies point sources.\n",
    "\n",
    "This package is known as photutils and we will be using DAOStarFinder from within that package. DAOStarFinder essentially looks for peaks above a certain threshold, say the background count level, and peaks of a certain size or full-width-at-half-maximum (FWHM). Take a look at the documentation below for more information:\n",
    "\n",
    "https://photutils.readthedocs.io/en/stable/api/photutils.detection.DAOStarFinder.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DAOPHOT to find sources in image\n",
    "Choose the V filter to find sources in first. We will be using DAOStarFinder to find sources in that image, but we first have to estimate the background level of our image. What is the mean, median, and standard deviation about the mean of you image? Print them out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V data\n",
    "vdata = fits.getdata('./data/science/d85_os_bs_ff_bp.fits') # your V filter science file\n",
    "\n",
    "# take statistics of the image to estimate the background\n",
    "mean, median, stddev = sigma_clipped_stats(vdata, sigma=3.0)\n",
    "\n",
    "# your print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the standard deviation (std) as a component of our threshold; let's say a good estimate for the threshold is some multiple of the standard deviation. Run the file below to find point sources (stars) in the image and print out information for each detection. Adjust the fwhm and threshold level. What do you notice as you change them? Do the number of detections increase or decrease?\n",
    "\n",
    "Try to get around 1,000-5,000 detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run source finder\n",
    "daofindv = DAOStarFinder(fwhm=8, threshold=1*std) # change these 0 parameters\n",
    "sourcesv = daofindv(vdata - median)  \n",
    "\n",
    "# print out list of detections\n",
    "for col in sourcesv.colnames:  \n",
    "    if col not in ('id', 'npix'):\n",
    "        sourcesv[col].info.format = '%.2f' \n",
    "sourcesv.pprint(max_width=76)  \n",
    "sourcesv.add_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, check location of sources on image. Does it look like we are finding real sources?\n",
    "If you are getting very few sources or obvious wrong detections, play around with the parameters above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5), dpi=300)\n",
    "ax.imshow(vdata, origin='lower', vmin=0, vmax=1000)\n",
    "ax.scatter(sourcesv['xcentroid'], sourcesv['ycentroid'], facecolors='none', edgecolors='r', lw=0.5, s=5)\n",
    "ax.set_xlabel('YOUR X LABEL')\n",
    "ax.set_ylabel('YOUR Y LABEL')\n",
    "ax.set_title('YOUR TITLE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use your detection list to find same sources in your other filter\n",
    "Having gotten detections in the V band, we need to match these to detection in the other filter we have, I. Input your file below. We will use the coordiates of detections in the V band and look for sources in the I filter image at the same location. As a note, we may not find as many sources because of the threshold considerations. \n",
    "\n",
    "Again, adjust the fwhm and threshold to see how many detections you can get. Aim for around 1,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sources in i-image \n",
    "\n",
    "Idata = fits.getdata('./data/science/d86_os_bs_ff_bp.fits') # put your I filter image here\n",
    "mean, median, std = sigma_clipped_stats(Idata, sigma=5.0)\n",
    "\n",
    "# xy coordinates of detections in the V-band that we will give DAOStarFinder\n",
    "xycoords = np.array(list(zip(sourcesv['xcentroid'], sourcesv['ycentroid'])))\n",
    "\n",
    "# Run source detection\n",
    "daofind_i = DAOStarFinder(fwhm=8, threshold=1*std, xycoords=xycoords) # change these numbers \n",
    "sourcesi = daofind_i(Idata-median)  \n",
    "\n",
    "# print out list of detections\n",
    "for col in sourcesi.colnames:  \n",
    "    if col not in ('id', 'npix'):\n",
    "        sourcesi[col].info.format = '%.2f' \n",
    "sourcesi.pprint(max_width=76)  \n",
    "sourcesi.add_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, check location of sources on I-band image. Does it look like we are finding real sources?\n",
    "If you are getting very few sources or obvious wrong detections, play around with the parameters above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check location on image\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5), dpi=300)\n",
    "ax.imshow(Idata, origin='lower', vmin=0, vmax=5000)\n",
    "ax.scatter(sourcesi['xcentroid'], sourcesi['ycentroid'], facecolors='none', edgecolors='r', lw=0.5, s=5)\n",
    "ax.set_xlabel('YOUR X AXIS')\n",
    "ax.set_ylabel('YOUR Y AXIS')\n",
    "ax.set_title('YOUR TITLE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we have to match sources between the two detection lists\n",
    "Not all of our sources are found in both filters. Therefore, we need to match between our list of stars for common matches to make our color-magnitude diagram. Run the cell below to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match between the two starlists\n",
    "\n",
    "matches = []\n",
    "for row in sourcesi:\n",
    "    # find nearest \n",
    "    nearest = np.min(np.abs((row['xcentroid'] - sourcesv['xcentroid'])))\n",
    "    idx = np.where(np.abs((row['xcentroid'] - sourcesv['xcentroid'])) == nearest)[0][0]\n",
    "    matches += [idx+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot in color-magnitude\n",
    "Below you will plot your Color-Magnitude diagram. In this diagram, think about the following:\n",
    "- On your diagram, where is the Main Sequence? \n",
    "- Which Main Sequence stars are bluer and more massive, and which stars are redder and less massive?\n",
    "- Do you see the Main Sequence turnoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to calculate the color and magnitude of each of your sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = sourcesi['mag']-sourcesv.loc[matches]['mag']\n",
    "mag = sourcesv.loc[matches]['mag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(YOURX, YOURY)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('YOURTITLE')\n",
    "plt.xlabel('YOURLABEL')\n",
    "plt.ylabel('YOURLABEL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the mass at the main sequence turnoff of our cluster\n",
    "Using the presense (or lack of a presense) of the turnoff, let's estimate the the most massive star in our cluster, and thus the mass of the turnoff stars. M92's age is around 14 billion years (if you choose a different, cluster look up it's age!)\n",
    "\n",
    "The relationship between the Mass of a star ($M$) and its Main Sequence Lifetime $(t)$ is given by the following formula:\n",
    "\n",
    "$t (\\mathrm{yr}) \\propto (4.5\\times 10^9 ) M^{-2.5} $\n",
    "\n",
    "Plug in this age estimate into the equation above. What is your estimated mass for your main sequence turnoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b05493ddcf8e9507683538249caa61aa01626b793e566d994882f595f5286b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
